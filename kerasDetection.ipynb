{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"kerasDetection.ipynb","provenance":[{"file_id":"1CuKQVoSWvCH_orNsxR7jg5YFGw1Hvu0O","timestamp":1561262309894}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Xma8LHnHnQIV"},"source":["####Analytics Vidya</b>\n","###Face detection and counting</p></b>\n","<p>People detection and head counting is one of the classical albeit challenging computer vision application. For this problem, given a group selfie/photo, you are required to count the number of heads present in the picture. You are provided with a training set of images with coordinates of bounding box and head count for each image and need to predict the headcount for each image in the test set.</p></br>"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pNqamX7xpIa_","executionInfo":{"status":"ok","timestamp":1607920082399,"user_tz":-330,"elapsed":26188,"user":{"displayName":"surya prakash","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp1g4sYLYO2ABLqq0IvWEeamc58nxfiqDAWGAc=s64","userId":"00371524238977310899"}},"outputId":"e3095ab0-6691-4e96-c531-48037a934407"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eDFEkL8lxPuf","executionInfo":{"status":"ok","timestamp":1607920096209,"user_tz":-330,"elapsed":39992,"user":{"displayName":"surya prakash","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp1g4sYLYO2ABLqq0IvWEeamc58nxfiqDAWGAc=s64","userId":"00371524238977310899"}}},"source":["import zipfile as zip\n","import os\n","\n","zip_ref = zip.ZipFile('./drive/My Drive/Colab Notebooks/AnalyticsVidya/train_HNzkrPW (1).zip', 'r')\n","zip_ref.extractall('train/')\n","zip_ref.close()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FazTJE_lnKGI","executionInfo":{"status":"ok","timestamp":1607920097762,"user_tz":-330,"elapsed":41538,"user":{"displayName":"surya prakash","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp1g4sYLYO2ABLqq0IvWEeamc58nxfiqDAWGAc=s64","userId":"00371524238977310899"}},"outputId":"0201ff43-11a6-4e40-d9e7-f34e0e905af5"},"source":["from keras.preprocessing.image import ImageDataGenerator\n","import pandas as pd\n","\n","IMG_SIZE = 256\n","BATCH_SIZE = 32\n","data_gen_args = dict(featurewise_center=True,featurewise_std_normalization=True,rescale=1./IMG_SIZE)\n","\n","traindatagen = ImageDataGenerator(**data_gen_args)\n","testdatagen = ImageDataGenerator(rescale=1./IMG_SIZE)\n","\n","traincsv = pd.read_csv('train/train.csv',dtype={'Name': str,'HeadCount':int})\n","trainFaceMaskcsv = pd.read_csv('train/bbox_train.csv',dtype={'Name': str,'width':float,'height':float,'xmin':float,'ymin':float,'xmax':float,'ymax':float})\n","testcsv = pd.read_csv('./drive/My Drive/Colab Notebooks/AnalyticsVidya/test.csv',dtype={'Name': str})\n","trainFaceMaskcsv['xmin'] = trainFaceMaskcsv['xmin']*IMG_SIZE/trainFaceMaskcsv['width']\n","trainFaceMaskcsv['xmax'] = trainFaceMaskcsv['xmax']*IMG_SIZE/trainFaceMaskcsv['width']\n","trainFaceMaskcsv['ymin'] = trainFaceMaskcsv['ymin']*IMG_SIZE/trainFaceMaskcsv['height']\n","trainFaceMaskcsv['ymax'] = trainFaceMaskcsv['ymax']*IMG_SIZE/trainFaceMaskcsv['height']\n","\n","traingenerator = traindatagen.flow_from_dataframe(traincsv, directory='train/image_data/',x_col='Name', y_col=['Name','HeadCount'], \n","                    target_size=(IMG_SIZE,IMG_SIZE), color_mode='rgb',class_mode='raw', batch_size=32,shuffle=False, seed=10)\n","\n","testgenerator = testdatagen.flow_from_dataframe(testcsv, directory='train/image_data/',x_col='Name', target_size=(IMG_SIZE,IMG_SIZE),color_mode='rgb', \n","                    class_mode='input',batch_size=32, shuffle=False, seed=None)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Found 5733 validated image filenames.\n","Found 2463 validated image filenames.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u1B2dxjLgnBE"},"source":["####Helper functions</b>"]},{"cell_type":"code","metadata":{"id":"4PQyY1SCPuK3","executionInfo":{"status":"ok","timestamp":1607920097764,"user_tz":-330,"elapsed":41536,"user":{"displayName":"surya prakash","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp1g4sYLYO2ABLqq0IvWEeamc58nxfiqDAWGAc=s64","userId":"00371524238977310899"}}},"source":["import numpy as np\n","def get_iou(a, epsilon=1e-5):\n","    a[:,8] = np.maximum(a[:,0], a[:,4])\n","    a[:,9] = np.maximum(a[:,1], a[:,5])\n","    a[:,10] = np.minimum(a[:,2], a[:,6])\n","    a[:,11] = np.minimum(a[:,3], a[:,7])\n","    overlap = np.multiply(np.maximum(0,np.subtract(a[:,10],a[:,8])),np.maximum(0,np.subtract(a[:,11],a[:,9])))\n","    area_a = np.multiply(np.subtract(a[:,2],a[:,0]),np.subtract(a[:,3],a[:,1]))\n","    area_b = np.multiply(np.subtract(a[:,6],a[:,4]),np.subtract(a[:,7],a[:,5]))\n","    area_combined = np.subtract(area_a+area_b,overlap)+epsilon\n","    a[:,12] = np.divide(overlap,area_combined)*IMG_SIZE\n","    return a"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"z01lq3S-eA3d","executionInfo":{"status":"ok","timestamp":1607920097766,"user_tz":-330,"elapsed":41536,"user":{"displayName":"surya prakash","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp1g4sYLYO2ABLqq0IvWEeamc58nxfiqDAWGAc=s64","userId":"00371524238977310899"}}},"source":["def get_faceMasks(batch_y,anchor):\n","  facemasks = np.zeros(shape=(batch_y.shape[0],int(IMG_SIZE/anchor),int(IMG_SIZE/anchor),6))\n","  for i in range(0,batch_y.shape[0]):\n","    faceList = trainFaceMaskcsv.where(trainFaceMaskcsv['Name']==batch_y[i,0])\n","    faceList = faceList[faceList.Name.notnull()]\n","    arr = []\n","    for face in range(faceList['Name'].count()):\n","      for row_x in range(0,IMG_SIZE,anchor):\n","        for col_y in range(0,IMG_SIZE,anchor): \n","          arr.append([row_x,col_y,row_x+anchor,col_y+anchor,faceList['xmin'].iloc[face],faceList['ymin'].iloc[face],faceList['xmax'].iloc[face],faceList['ymax'].iloc[face],0,0,0,0,0,faceList['Name'].count()])\n","    ious = get_iou(np.array(arr))\n","    ious2 = pd.DataFrame(ious)\n","    idx = ious2.loc[ious2.reset_index().groupby([0,1,2,3])[12].idxmax()]\n","    facemasks[i] = np.reshape((np.array(idx)/IMG_SIZE)[:,[8,9,10,11,12,13]],newshape=(int(IMG_SIZE/anchor),int(IMG_SIZE/anchor),6))\n","  return facemasks"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xykofu3AanLo"},"source":["####Test Pre</b>"]},{"cell_type":"code","metadata":{"id":"2U-pmFtXwBf9","executionInfo":{"status":"ok","timestamp":1607920097767,"user_tz":-330,"elapsed":41535,"user":{"displayName":"surya prakash","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp1g4sYLYO2ABLqq0IvWEeamc58nxfiqDAWGAc=s64","userId":"00371524238977310899"}}},"source":["import numpy as np\n","\n","def get_output(batch):\n","  while True:\n","    batch_x, batch_y = next(batch)\n","    yield (batch_x, [get_faceMasks(batch_y,8),get_faceMasks(batch_y,16),get_faceMasks(batch_y,32),get_faceMasks(batch_y,64),get_faceMasks(batch_y,128)])"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"id":"op7MIasIL-ea","executionInfo":{"status":"error","timestamp":1607920121137,"user_tz":-330,"elapsed":64899,"user":{"displayName":"surya prakash","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp1g4sYLYO2ABLqq0IvWEeamc58nxfiqDAWGAc=s64","userId":"00371524238977310899"}},"outputId":"f4593b81-98b7-4a6f-9aff-78cb1e42f362"},"source":["from matplotlib import pyplot as plt\n","import matplotlib.patches as patches\n","\n","x_train,y_train = next(get_output(traingenerator))\n","for pic in range(7):\n","  im = np.array(x_train[pic*4])\n","  fig,ax = plt.subplots(pic,figsize = (8, 8))\n","  ax.imshow(im)\n","  for anchor in range(len(y_train)):\n","    boxes = np.reshape(y_train[30][anchor],newshape=(int((IMG_SIZE*IMG_SIZE)/(anchor_sizes[anchor]*anchor_sizes[anchor])),13))\n","    boxes = boxes[boxes[:,12]>0]*IMG_SIZE\n","    for box in range(len(boxes)):\n","      ax.add_patch(patches.Rectangle((boxes[box,8],boxes[box,9]),boxes[box,10]-boxes[box,8],boxes[box,11]-boxes[box,9],linewidth=1,edgecolor='r',facecolor='none'))\n","plt.show()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n","  warnings.warn('This ImageDataGenerator specifies '\n","/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n","  warnings.warn('This ImageDataGenerator specifies '\n"],"name":"stderr"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-103b3f7095ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0manchor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnewshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0manchor_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'imshow'"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 576x576 with 0 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"LV4r9SJdrRXQ","executionInfo":{"status":"ok","timestamp":1607920175373,"user_tz":-330,"elapsed":1390,"user":{"displayName":"surya prakash","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp1g4sYLYO2ABLqq0IvWEeamc58nxfiqDAWGAc=s64","userId":"00371524238977310899"}}},"source":["import tensorflow as tf\n","from keras.layers import (Flatten,LeakyReLU,AveragePooling2D,concatenate,\n","                                            Reshape,Activation,Input, Conv2D, MaxPooling2D,\n","                                            BatchNormalization, GlobalAveragePooling2D,SpatialDropout2D)\n","from keras import Model\n","from tensorflow.keras.layers import Lambda\n","def resNetBlock(input_image,num):\n","\n","  layer1 = Conv2D(128, 3, strides=(1,1), padding='same', use_bias=False,name ='rone'+str(num))(input_image)\n","  layer1 = BatchNormalization()(layer1)\n","  layer1 = SpatialDropout2D(0.25)(layer1)\n","  layer1 = LeakyReLU(alpha=0.1)(layer1)\n","  \n","  layer3 = Conv2D(64, 1, strides=(1,1), padding='same', use_bias=False,name='rtwo'+str(num))(layer1)\n","  layer3 = BatchNormalization()(layer3)\n","  layer3 = SpatialDropout2D(0.25)(layer3)\n","  layer3 = LeakyReLU(alpha=0.1)(layer3)\n","    \n","  return layer3  \n","\n","def build_model(img_size):\n","  input_sh = Input(shape=(img_size,img_size,3)) \n","\n","  layer_1 = Conv2D(16, 3, strides=(1,1), padding='same', use_bias=False,name='one')(input_sh)\n","  layer_1 = BatchNormalization()(layer_1)\n","  layer_1 = SpatialDropout2D(0.25)(layer_1)\n","  layer_1 = LeakyReLU(alpha=0.1)(layer_1)\n","  \n","  layer_2 = Conv2D(32, 3, strides=(1,1), padding='same', use_bias=False,name='two')(layer_1)\n","  layer_2 = BatchNormalization()(layer_2)\n","  layer_2 = SpatialDropout2D(0.25)(layer_2)\n","  layer_2 = LeakyReLU(alpha=0.1)(layer_2)\n","  \n","  layer_3 = Conv2D(64, 3, strides=(1,1), padding='same', use_bias=False,name='three')(layer_2)\n","  layer_3 = BatchNormalization()(layer_3)\n","  layer_3 = SpatialDropout2D(0.25)(layer_3)\n","  layer_3 = LeakyReLU(alpha=0.1)(layer_3)\n"," \n","  resNetBlock1 = resNetBlock(layer_3,1)\n","  concact1 = concatenate([resNetBlock1, layer_3])\n","  layer_l = LeakyReLU(alpha=0.1)(concact1)\n","  maxpool1 = MaxPooling2D(pool_size=(2, 2))(layer_l)\n","\n","  resNetBlock2 = resNetBlock(maxpool1,2)\n","  outputy = Conv2D(8, 1, strides=(1,1), padding='same', use_bias=False,name='four')(resNetBlock2)\n","  concact2 = concatenate([resNetBlock2, maxpool1])\n","  layer_l2 = LeakyReLU(alpha=0.1)(concact2)\n","  maxpool2 = MaxPooling2D(pool_size=(2, 2))(layer_l2)\n","\n","  resNetBlock3 = resNetBlock(maxpool2,3)\n","  outputz = Conv2D(16, 1, strides=(1,1), padding='same', use_bias=False,name='five')(resNetBlock3)\n","  concact3 = concatenate([resNetBlock3, maxpool2])\n","  layer_l3 = LeakyReLU(alpha=0.1)(concact3)\n","  maxpool3 = MaxPooling2D(pool_size=(2, 2))(layer_l3)\n","  \n","  resNetBlock4 = resNetBlock(maxpool3,4)\n","  outputw = Conv2D(32, 1, strides=(1,1), padding='same', use_bias=False,name='six')(resNetBlock4)\n","  concact4 = concatenate([resNetBlock4, maxpool3])\n","  layer_l4 = LeakyReLU(alpha=0.1)(concact4)\n","  maxpool4 = MaxPooling2D(pool_size=(2, 2))(layer_l4)\n","  \n","  resNetBlock5 = resNetBlock(maxpool4,5)\n","  outputk = Conv2D(6, 1, strides=(1,1), padding='same', use_bias=False,name='seven')(resNetBlock5)\n","  concact5 = concatenate([resNetBlock5, maxpool4])\n","  layer_l5 = LeakyReLU(alpha=0.1)(concact5)\n","  maxpool5 = MaxPooling2D(pool_size=(2, 2))(layer_l5)\n","  \n","  resNetBlock6 = resNetBlock(maxpool5,6)\n","  concact6 = concatenate([resNetBlock6, maxpool5])\n","  layer_l6 = LeakyReLU(alpha=0.1)(concact6)\n","\n","  layer_ant = Conv2D(6, (1,1), strides=(1,1), padding='same', use_bias=False, name='eight')(layer_l6)\n","  layer_plus1 = Conv2D(6, (3,3), strides=(1,1),  use_bias=False, name='nine')(layer_ant)\n","  layer_plus2 = Conv2D(6, (3,3), strides=(1,1), use_bias=False, name='ten')(layer_plus1)\n","  layer_plus3 = Conv2D(6, (3,3), strides=(1,1),  use_bias=False, name='eleven')(layer_plus2)\n","  model = Model(input_sh,outputs=[outputw,outputk,layer_ant,layer_plus2,layer_plus3])\n","  return model"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"pqGFM7p8sHv1","executionInfo":{"status":"ok","timestamp":1607920175879,"user_tz":-330,"elapsed":1887,"user":{"displayName":"surya prakash","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp1g4sYLYO2ABLqq0IvWEeamc58nxfiqDAWGAc=s64","userId":"00371524238977310899"}}},"source":["import tensorflow_addons as tfa\n","def my_loss5(y_true,y_pred):\n","  loss_classification = tf.math.abs(tf.math.subtract(y_true[:,:,:,4],y_pred[:,:,:,4]))\n","  loss_regression = tf.math.abs(tf.math.subtract(y_true[:,:,:,5],y_pred[:,:,:,5]))\n","  gl = tfa.losses.GIoULoss()\n","  loss = tf.math.reduce_sum(loss_classification)+tf.math.reduce_sum(loss_regression)+tf.math.reduce_sum(gl(y_true[:,:,:,0:4], y_pred[:,:,:,0:4]))\n","  return loss"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"hqvK9GuPN0p4","executionInfo":{"status":"ok","timestamp":1607920182665,"user_tz":-330,"elapsed":8670,"user":{"displayName":"surya prakash","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp1g4sYLYO2ABLqq0IvWEeamc58nxfiqDAWGAc=s64","userId":"00371524238977310899"}}},"source":["current_model= build_model(256)\n","current_model.load_weights('drive/My Drive/Colab Notebooks/AnalyticsVidya/FaceDetection/Weights/face_detection.hdf5')"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"5UkOAJ9rVV__","executionInfo":{"status":"ok","timestamp":1607920182666,"user_tz":-330,"elapsed":8667,"user":{"displayName":"surya prakash","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp1g4sYLYO2ABLqq0IvWEeamc58nxfiqDAWGAc=s64","userId":"00371524238977310899"}}},"source":["from tensorflow.python.keras.callbacks import ModelCheckpoint\n","\n","current_model.compile(loss={'six':my_loss5,'seven':my_loss5,'eight':my_loss5,'ten':my_loss5,'eleven':my_loss5} ,\n","              optimizer='adam',metrics=['accuracy'])"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HhQo5KNR0pup","outputId":"f817004e-dee7-4960-b764-44ed4d22ee44"},"source":["from tensorflow.python.keras.callbacks import ModelCheckpoint\r\n","checkpoint = ModelCheckpoint('drive/My Drive/Colab Notebooks/AnalyticsVidya/FaceDetection/Weights/face_detection.hdf5',monitor='eleven_accuracy',\r\n","                             save_weights_only=False,save_best_only = True,mode='auto')\r\n","current_model.fit(get_output(traingenerator),epochs=50,verbose=1,steps_per_epoch=int(5733/32),callbacks=[checkpoint])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n","  warnings.warn('This ImageDataGenerator specifies '\n","/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n","  warnings.warn('This ImageDataGenerator specifies '\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/50\n","179/179 [==============================] - 4359s 24s/step - loss: 452.4395 - six_loss: 153.2225 - seven_loss: 118.5342 - eight_loss: 141.3259 - ten_loss: 31.9657 - eleven_loss: 7.3912 - six_accuracy: 0.3968 - seven_accuracy: 0.1006 - eight_accuracy: 0.2076 - ten_accuracy: 0.1738 - eleven_accuracy: 0.5034\n","Epoch 2/50\n","179/179 [==============================] - 4400s 25s/step - loss: 427.2462 - six_loss: 150.7112 - seven_loss: 116.9425 - eight_loss: 121.1735 - ten_loss: 31.1451 - eleven_loss: 7.2738 - six_accuracy: 0.3958 - seven_accuracy: 0.0914 - eight_accuracy: 0.2078 - ten_accuracy: 0.1726 - eleven_accuracy: 0.5055\n","Epoch 3/50\n","179/179 [==============================] - 4385s 24s/step - loss: 416.9950 - six_loss: 149.1205 - seven_loss: 117.8558 - eight_loss: 111.8520 - ten_loss: 31.0441 - eleven_loss: 7.1226 - six_accuracy: 0.3896 - seven_accuracy: 0.0929 - eight_accuracy: 0.2076 - ten_accuracy: 0.1712 - eleven_accuracy: 0.5057\n","Epoch 4/50\n","179/179 [==============================] - 4379s 24s/step - loss: 395.3303 - six_loss: 141.4234 - seven_loss: 116.6281 - eight_loss: 99.1103 - ten_loss: 31.1781 - eleven_loss: 6.9903 - six_accuracy: 0.4070 - seven_accuracy: 0.0860 - eight_accuracy: 0.2079 - ten_accuracy: 0.1714 - eleven_accuracy: 0.5054\n","Epoch 5/50\n","179/179 [==============================] - 4347s 24s/step - loss: 384.9265 - six_loss: 136.7387 - seven_loss: 116.3897 - eight_loss: 93.9255 - ten_loss: 30.9706 - eleven_loss: 6.9022 - six_accuracy: 0.3893 - seven_accuracy: 0.0884 - eight_accuracy: 0.2072 - ten_accuracy: 0.1703 - eleven_accuracy: 0.5068\n","Epoch 6/50\n","179/179 [==============================] - 4369s 24s/step - loss: 381.1743 - six_loss: 136.4481 - seven_loss: 116.7231 - eight_loss: 90.2572 - ten_loss: 30.9727 - eleven_loss: 6.7733 - six_accuracy: 0.3856 - seven_accuracy: 0.0871 - eight_accuracy: 0.2075 - ten_accuracy: 0.1703 - eleven_accuracy: 0.5057\n","Epoch 7/50\n","179/179 [==============================] - 4314s 24s/step - loss: 377.1964 - six_loss: 135.2117 - seven_loss: 116.6415 - eight_loss: 87.8492 - ten_loss: 30.7830 - eleven_loss: 6.7110 - six_accuracy: 0.4165 - seven_accuracy: 0.0858 - eight_accuracy: 0.2077 - ten_accuracy: 0.1707 - eleven_accuracy: 0.5061\n","Epoch 8/50\n","179/179 [==============================] - 4326s 24s/step - loss: 380.1745 - six_loss: 136.8296 - seven_loss: 118.2112 - eight_loss: 87.5907 - ten_loss: 30.8139 - eleven_loss: 6.7290 - six_accuracy: 0.4197 - seven_accuracy: 0.0931 - eight_accuracy: 0.2086 - ten_accuracy: 0.1747 - eleven_accuracy: 0.5066\n","Epoch 9/50\n"," 38/179 [=====>........................] - ETA: 57:25 - loss: 382.7790 - six_loss: 135.8462 - seven_loss: 116.3439 - eight_loss: 91.6776 - ten_loss: 31.8670 - eleven_loss: 7.0443 - six_accuracy: 0.3895 - seven_accuracy: 0.0906 - eight_accuracy: 0.2156 - ten_accuracy: 0.1778 - eleven_accuracy: 0.5088"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b8Aj5Q8iP7AW"},"source":["from matplotlib import pyplot as plt\n","from PIL import Image,ImageOps,ImageFilter\n","import matplotlib.patches as patches\n","import random\n","import cv2\n","from google.colab.patches import cv2_imshow\n","\n","face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n","profile_cascade = cv2.CascadeClassifier('haarcascade_profileface.xml')\n","eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n","item_num = random.randint(0,(x_ou[0].shape[0])-1)\n","#item_num = 24\n","img = Image.fromarray((x_ou[0][item_num]).astype('uint8'),mode='RGB')\n","print(item_num)\n","\n","fig = plt.figure()\n","ax = fig.add_subplot(1,1,1)\n","ticks = np.arange(0, 256, 32)\n","\n","ax.set_xticks(ticks)\n","ax.set_yticks(ticks)\n","ax.grid(b=True,color='black')\n","gray = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2GRAY)\n","\n","faces = face_cascade.detectMultiScale(gray, 1.01, 10)\n","for (x1,y1,w1,h1) in faces:\n","    print('x1 '+str(x1)+' y1 '+str(y1)+' w1 '+str(w1)+' h1 '+str(h1))\n","    img = cv2.rectangle(np.array(img),(x1,y1),(x1+w1,y1+h1),(255,0,0),2)\n","    roi_color = img[y1:y1+h1, x1:x1+w1]\n","\n","profiles = profile_cascade.detectMultiScale(gray, 1.01, 10)\n","for (x2,y2,w2,h2) in profiles:\n","    print('x2 '+str(x2)+' y2 '+str(y2)+' w2 '+str(w2)+' h2 '+str(h2))\n","    img = cv2.rectangle(np.array(img),(x2,y2),(x2+w2,y2+h2),(0,255,0),2)\n","    roi_color = img[y2:y2+h2, x2:x2+w2]\n","\n","ax.imshow(img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s6LHBS_gh7o5"},"source":["####Model</b>"]},{"cell_type":"markdown","metadata":{"id":"CUZE-tU83JTy"},"source":["16 x 16 x 5 x 2"]},{"cell_type":"code","metadata":{"id":"6CnKGl6rOyAe"},"source":["import requests\n","from io import BytesIO\n","import numpy as np\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","\n","\n","response = requests.get('https://www.bmw.co.uk/bmw-cars/explore-the-range/lifestyle/image-thumb__14913__480x270/3-series-1920x1080.jpeg?1539010591')\n","img = Image.open(BytesIO(response.content))\n","img2 = img.resize((32, 32), Image.NEAREST) \n","img3 =np.expand_dims(img2, axis=0)\n","print(img3.shape)\n","p = current_model.predict(img3)\n","print(p.shape)\n","print(p[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9KQjeOFnM_ja"},"source":["def fetch_model(input_size):\n","  files = os.listdir('/drive/My Drive/Colab Notebooks/AnalyticsVidya/FaceDetection')\n","\n","  matchs = [s for s in files if SAVED_MODEL in s]\n","  \n","  least_val_loss = 1000.00\n","  previous_model_file = ''\n","  for m in matchs:\n","      if float((m.split('_',2)[2]).split('.hdf5',1)[0]) < least_val_loss:\n","        least_val_loss = float((m.split('_',2)[2]).split('.hdf5',1)[0])\n","        previous_model_file = '/drive/My Drive/Colab Notebooks/AnalyticsVidya/FaceDetection' + m\n","  if(least_val_loss < 1000.00):\n","      print(\"Loading previous model with least val loss: \"+ str(previous_model_file))\n","      current_model = build_model(img_size=input_size)\n","      current_model.load_weights(previous_model_file)\n","  else:\n","      print(\"No previous model..   creating new Model\")\n","      current_model = build_model(img_size=input_size)\n","  \n","  return current_model"],"execution_count":null,"outputs":[]}]}